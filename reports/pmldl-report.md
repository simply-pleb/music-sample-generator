# Practical Machine Learning and Deep Learning - Final Technical Report

## Team

Name of the team: Suyeta

Team members:

- Robert Chen (r.chen@innopolis.university)
- Ahmadsho Akdodshoev (a.akdodshoev@innopolis.university)

## Project topic

Music Sample Generation

## Link to the repository

Repository: [https://github.com/simply-pleb/music-sample-generator](https://github.com/simply-pleb/music-sample-generator)

## What you tried to do during the project

We tried to generate ~10s music clips, that are called **samples** in the music industry, for a specific use case described below.

Samples are widely used in contemporary music production. A sample should have a catchy melody or an interesting "texture". They are later mixed with an original melody in order to produce a track.

We chose to train a model to generate samples in the Soul genre, since it is popular among produces.

## Main results (including artifacts)

## Timeline of the project

September

October 

November

## Individual contributions of the teammates

- Robert Chen: model implementation
- Ahmadsho Akdodshoev: literature review, data collection and data preparation  

## References

| Title and demo page | Paper | Code |
| - | - | - |
| [Noise2Music: Text-conditioned Music Generation with Diffusion Models](https://google-research.github.io/noise2music/) | [arXiv](https://arxiv.org/abs/2302.03917) | |
| [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/)| [arXiv](https://arxiv.org/abs/2301.11325)| [GitHub (unofficial)](https://github.com/lucidrains/musiclm-pytorch) |
| [MusicGen: Simple and Controllable Music Generation](https://ai.honu.io/papers/musicgen/)| [arXiv](https://arxiv.org/abs/2306.05284) | [Github](https://github.com/facebookresearch/audiocraft) |
| [Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion](https://anonymous0.notion.site/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc)| [arXiv](https://arxiv.org/abs/2301.11757) | [GitHub](https://github.com/archinetai/audio-diffusion-pytorch) |
| [Msanii: High Fidelity Music Synthesis on a Shoestring Budget](https://kinyugo.github.io/msanii-demo/)| [arXiv](https://arxiv.org/abs/2301.06468) | [GitHub](https://github.com/Kinyugo/msanii) |
| [JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models](https://www.futureverse.com/research/jen/demos/jen1)| [arXiv](https://arxiv.org/abs/2308.04729)| |
| [MeLoDy: Efficient Neural Music Generation](https://efficient-melody.github.io/)| [arXiv](https://arxiv.org/abs/2305.15719) | |