{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training MusicLM model\n","### Robert Chen, Ahmadsho Akdodshoev, Philip Timofeev"]},{"cell_type":"markdown","metadata":{},"source":["## 0. Imports"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:20.342753Z","iopub.status.busy":"2023-11-23T03:23:20.342228Z","iopub.status.idle":"2023-11-23T03:23:35.140123Z","shell.execute_reply":"2023-11-23T03:23:35.138728Z","shell.execute_reply.started":"2023-11-23T03:23:20.342712Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: musiclm-pytorch in /opt/conda/lib/python3.10/site-packages (0.2.8)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.24.1)\n","Requirement already satisfied: audiolm-pytorch>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (1.8.2)\n","Requirement already satisfied: beartype in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.16.4)\n","Requirement already satisfied: einops>=0.6 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.7.0)\n","Requirement already satisfied: lion-pytorch in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.1.2)\n","Requirement already satisfied: vector-quantize-pytorch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (1.11.7)\n","Requirement already satisfied: x-clip in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (0.14.4)\n","Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.0+cpu)\n","Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from musiclm-pytorch) (2.0.1+cpu)\n","Requirement already satisfied: ema-pytorch>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.1)\n","Requirement already satisfied: encodec in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.1)\n","Requirement already satisfied: fairseq in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.12.2)\n","Requirement already satisfied: gateloop-transformer>=0.0.24 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.0)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.3.2)\n","Requirement already satisfied: local-attention>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.9.0)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.2.2)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.1.99)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.33.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.66.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (6.0)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate->musiclm-pytorch) (0.16.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (4.6.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->musiclm-pytorch) (3.1.2)\n","Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (6.1.3)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (2023.6.3)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from x-clip->musiclm-pytorch) (0.15.1+cpu)\n","Requirement already satisfied: rotary-embedding-torch in /opt/conda/lib/python3.10/site-packages (from gateloop-transformer>=0.0.24->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.5)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->musiclm-pytorch) (3.0.9)\n","Requirement already satisfied: cffi in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.15.1)\n","Requirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.29.35)\n","Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.0.7)\n","Requirement already satisfied: omegaconf<2.1 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.0.6)\n","Requirement already satisfied: sacrebleu>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.3.2)\n","Requirement already satisfied: bitarray in /opt/conda/lib/python3.10/site-packages (from fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.12)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->musiclm-pytorch) (2023.9.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->musiclm-pytorch) (2.31.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->musiclm-pytorch) (2.1.3)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (1.11.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch>=0.17.0->musiclm-pytorch) (3.1.0)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->musiclm-pytorch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->x-clip->musiclm-pytorch) (9.5.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.3.3)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /opt/conda/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.8)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.8.2)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.9.0)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (4.9.3)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi->fairseq->audiolm-pytorch>=0.17.0->musiclm-pytorch) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->musiclm-pytorch) (2023.7.22)\n"]}],"source":["!pip install musiclm-pytorch"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:35.144251Z","iopub.status.busy":"2023-11-23T03:23:35.143678Z","iopub.status.idle":"2023-11-23T03:23:35.152791Z","shell.execute_reply":"2023-11-23T03:23:35.151601Z","shell.execute_reply.started":"2023-11-23T03:23:35.144193Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchaudio\n","from musiclm_pytorch import MuLaN, MuLaNEmbedQuantizer, \\\n","                            AudioSpectrogramTransformer, TextTransformer, MusicLM\n","from audiolm_pytorch import SemanticTransformer, SemanticTransformerTrainer, \\\n","                            CoarseTransformer, CoarseTransformerTrainer, \\\n","                            FineTransformer, FineTransformerTrainer, \\\n","                            AudioLM, HubertWithKmeans, MusicLMSoundStream, \\\n","                            SoundStreamTrainer, SoundStream \n","import os\n","from scipy.io.wavfile import read as read_wav\n","import urllib.request\n","import pandas as pd\n","import numpy as np\n","import audio2numpy as a2n\n","from x_clip.tokenizer import tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Creating dataloaders and downloading Hubert K-means checkpoints"]},{"cell_type":"markdown","metadata":{},"source":["Creating the dataset"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:35.155619Z","iopub.status.busy":"2023-11-23T03:23:35.154254Z","iopub.status.idle":"2023-11-23T03:23:35.589124Z","shell.execute_reply":"2023-11-23T03:23:35.587707Z","shell.execute_reply.started":"2023-11-23T03:23:35.155579Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 10])\n"]}],"source":["dataset_path = '/kaggle/input/musiclm-test/music-lm/data/dataset.tsv'\n","audio_path = '/kaggle/input/musiclm-test/music-lm/data/'\n","\n","\n","class MusicLMDataset(Dataset):\n","    def __init__(self, path: str):\n","        self.df = pd.read_csv(path, sep='\\t')\n","        self.filenames = list(map(lambda x: audio_path + x, self.df['filename']))\n","        self.authors = self.df['author']\n","        self.years = self.df['year']\n","    def __getitem__(self, idx):\n","        return torch.tensor(a2n.audio_from_file(self.filenames[idx])[0]), tokenizer.tokenize([self.authors[idx], self.years[idx]]).reshape(-1)\n","    def __len__(self):\n","        return len(self.filenames)\n","    \n","train_dataset = MusicLMDataset(dataset_path)\n","train_dataloader = DataLoader(train_dataset, batch_size=3)\n","tmp = next(iter(train_dataloader))\n","print(tmp[1].shape)"]},{"cell_type":"markdown","metadata":{},"source":["Downloading Hubert checkpoints"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:35.592498Z","iopub.status.busy":"2023-11-23T03:23:35.591778Z","iopub.status.idle":"2023-11-23T03:23:35.600126Z","shell.execute_reply":"2023-11-23T03:23:35.598893Z","shell.execute_reply.started":"2023-11-23T03:23:35.592452Z"},"trusted":true},"outputs":[],"source":["hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n","hubert_quantizer = 'hubert/hubert_base_ls960_L9_km500.bin'\n","soundstream_ckpt = './results/soundstream.pt'\n","mulan_ckpt = './results/mulan.pt'\n","\n","if not os.path.isdir(\"hubert\"):\n","  os.makedirs(\"hubert\")\n","if not os.path.isfile(hubert_ckpt):\n","  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n","  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n","if not os.path.isfile(hubert_quantizer):\n","  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n","  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Training MuLaN"]},{"cell_type":"markdown","metadata":{},"source":["Arguments for every module are defined in the respective dictionaries to make fine-tuning easier"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:35.602470Z","iopub.status.busy":"2023-11-23T03:23:35.601411Z","iopub.status.idle":"2023-11-23T03:23:35.614788Z","shell.execute_reply":"2023-11-23T03:23:35.613777Z","shell.execute_reply.started":"2023-11-23T03:23:35.602433Z"},"trusted":true},"outputs":[],"source":["AUDIO_KWARGS = {\n","    'dim': 512,\n","    'depth': 6,\n","    'heads': 8,\n","    'accept_spec': True,\n","    'dim_head': 64,\n","    'spec_n_fft': 128,\n","    'spec_win_length': 24,\n","    'spec_aug_stretch_factor': 0.8,\n","    'patch_dropout_prob': 0.\n","}\n","\n","TEXT_KWARGS = {\n","    'dim': 512,\n","    'depth': 6,\n","    'heads': 8,\n","    'dim_head': 64\n","}\n","\n","MULAN_KWARGS = {\n","    'dataset': train_dataset,\n","    'num_train_steps': 10,\n","    'batch_size': 16,\n","    'force_clear_prev_results': False,\n","    'save_model_every': 5\n","}\n","\n","MULAN_QUANTIZER_KWARGS = {\n","    'conditioning_dims': (1024, 1024, 1024),\n","    'namespaces': ('semantic', 'coarse', 'fine')\n","}\n","\n","HUBERT_KWARGS = {\n","    'checkpoint_path': hubert_ckpt,\n","    'kmeans_path': hubert_quantizer\n","}\n","\n","SOUNDSTREAM_TRAINER_KWARGS = {\n","    'folder': audio_path,\n","    'num_train_steps': 20,\n","    'save_model_every': 2,\n","    'batch_size': 4,\n","    'data_max_length_seconds': 60\n","}\n","    \n","SEMANTIC_KWARGS = {\n","    'dim': 1024,\n","    'depth': 6,\n","    'audio_text_condition': True \n","}\n","\n","COARSE_KWARGS = {\n","    'codebook_size': 1024,\n","    'num_coarse_quantizers': 4,\n","    'dim': 1024,\n","    'depth': 6,\n","    'audio_text_condition': True \n","}\n","\n","FINE_KWARGS = {\n","    'codebook_size': 1024,\n","    'num_coarse_quantizers': 4,\n","    'num_fine_quantizers': 8,\n","    'dim': 1024,\n","    'depth': 6,\n","    'audio_text_condition': True \n","}\n","\n","TRANSFORMER_TRAINER_KWARGS = {\n","    'folder': audio_path,\n","    'num_train_steps': 10,\n","    'save_model_every': 2,\n","    'batch_size': 4,\n","    'data_max_length_seconds': 30\n","}\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["Training MuLaN"]},{"cell_type":"code","execution_count":76,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-11-23T03:23:35.617303Z","iopub.status.busy":"2023-11-23T03:23:35.616613Z","iopub.status.idle":"2023-11-23T03:23:35.678069Z","shell.execute_reply":"2023-11-23T03:23:35.676781Z","shell.execute_reply.started":"2023-11-23T03:23:35.617257Z"},"trusted":true},"outputs":[],"source":["import copy\n","from math import sqrt\n","from random import choice\n","from pathlib import Path\n","from shutil import rmtree\n","from functools import wraps, partial\n","\n","from typing_extensions import Annotated\n","\n","from beartype import beartype\n","from beartype.door import is_bearable\n","from beartype.vale import Is\n","from beartype.typing import Union, List, Optional, Tuple, Callable\n","\n","from torch import nn\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","from lion_pytorch import Lion\n","\n","from musiclm_pytorch import MuLaN\n","\n","from einops import rearrange\n","\n","from accelerate import Accelerator\n","\n","# for automatically routing data emitted from a dataset to keywords of the transformer wrappers\n","\n","DATASET_FIELD_TYPE_CONFIG = dict(\n","    wavs = Annotated[\n","        torch.Tensor,\n","        Is[lambda t: t.dtype == torch.float and t.ndim in {2, 3}]\n","    ],\n","    raw_texts = List[str],\n","    texts = Annotated[\n","        torch.Tensor,\n","        Is[lambda t: t.dtype == torch.long and t.ndim == 2]\n","    ],\n",")\n","\n","# helpers\n","\n","def exists(val):\n","    return val is not None\n","\n","def default(*args):\n","    for arg in args:\n","        if exists(arg):\n","            return arg\n","    return None\n","\n","def noop(*args, **kwargs):\n","    pass\n","\n","def cycle(dl):\n","    while True:\n","        for data in dl:\n","            yield data\n","\n","def cast_tuple(t):\n","    return t if isinstance(t, (tuple, list)) else (t,)\n","\n","def yes_or_no(question):\n","    answer = input(f'{question} (y/n) ')\n","    return answer.lower() in ('yes', 'y')\n","\n","def accum_log(log, new_logs):\n","    for key, new_value in new_logs.items():\n","        old_value = log.get(key, 0.)\n","        log[key] = old_value + new_value\n","    return log\n","\n","# auto data to module keyword argument routing functions\n","\n","def has_duplicates(tup):\n","    counts = dict()\n","    for el in tup:\n","        if el not in counts:\n","            counts[el] = 0\n","        counts[el] += 1\n","    return any(filter(lambda count: count > 1, counts.values()))\n","\n","def determine_types(data, config):\n","    output = []\n","    for el in data:\n","        for name, data_type in config.items():\n","            if is_bearable(el, data_type):\n","                output.append(name)\n","                break\n","        else:\n","            raise TypeError(f'unable to determine type of {data}')\n","\n","    return tuple(output)\n","\n","# optimizer functions\n","\n","def separate_weight_decayable_params(params):\n","    wd_params, no_wd_params = [], []\n","    for param in params:\n","        param_list = no_wd_params if param.ndim < 2 else wd_params\n","        param_list.append(param)\n","    return wd_params, no_wd_params\n","\n","# dataloader functions\n","\n","def collate_one_or_multiple_tensors(fn):\n","    @wraps(fn)\n","    def inner(data):\n","        is_one_data = not isinstance(data[0], tuple)\n","\n","        if is_one_data:\n","            data = torch.stack(data)\n","            return (data,)\n","\n","        outputs = []\n","        for datum in zip(*data):\n","            if is_bearable(datum, Tuple[str, ...]):\n","                output = list(datum)\n","            else:\n","                output = fn(datum)\n","\n","            outputs.append(output)\n","\n","        return tuple(outputs)\n","\n","    return inner\n","\n","@collate_one_or_multiple_tensors\n","def curtail_to_shortest_collate(data):\n","    min_len = min(*[datum.shape[0] for datum in data])\n","    data = [datum[:min_len] for datum in data]\n","    return torch.stack(data)\n","\n","@collate_one_or_multiple_tensors\n","def pad_to_longest_fn(data):\n","    return pad_sequence(data, batch_first = True)\n","\n","def get_dataloader(ds, pad_to_longest = True, **kwargs):\n","    collate_fn = pad_to_longest_fn if pad_to_longest else curtail_to_shortest_collate\n","    return DataLoader(ds, collate_fn = collate_fn, **kwargs)\n","\n","# semantic transformer trainer\n","\n","@beartype\n","class MuLaNTrainer(nn.Module):\n","    def __init__(\n","        self,\n","        mulan: MuLaN,\n","        dataset: Dataset,\n","        *,\n","        num_train_steps = None,\n","        batch_size,\n","        data_max_length = None,\n","        folder = None,\n","        lr = 3e-4,\n","        grad_accum_every = 1,\n","        betas = (0.9, 0.99),\n","        max_grad_norm = 0.5,\n","        valid_frac = 0.05,\n","        random_split_seed = 42,\n","        save_model_every = 1000,\n","        results_folder = './results',\n","        accelerate_kwargs: dict = dict(),\n","        use_lion = False,\n","        force_clear_prev_results = None  # set to True | False to skip the prompt\n","    ):\n","        super().__init__()\n","        assert batch_size > 1, 'batch size must be greater than 1 for contrastive learning (but ideally as large as possible)'\n","\n","        self.accelerator = Accelerator(**accelerate_kwargs)\n","\n","        self.mulan = mulan\n","\n","        self.register_buffer('steps', torch.Tensor([0]))\n","\n","        self.num_train_steps = default(num_train_steps, len(dataset)) # 1 epoch by default\n","        self.batch_size = batch_size\n","        self.grad_accum_every = grad_accum_every\n","\n","        # optimizers\n","\n","        optim_klass = Lion if use_lion else Adam\n","        self.optim = optim_klass(mulan.parameters(), lr = lr, betas = betas)\n","\n","        # max grad norm\n","\n","        self.max_grad_norm = max_grad_norm\n","\n","        self.data_max_length = data_max_length\n","\n","        # create dataset\n","\n","        self.ds = dataset\n","        self.ds_fields = None\n","\n","        # split for validation\n","\n","        if valid_frac > 0:\n","            train_size = int((1 - valid_frac) * len(self.ds))\n","            valid_size = len(self.ds) - train_size\n","            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n","            self.print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n","        else:\n","            self.valid_ds = self.ds\n","            self.print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n","\n","        # dataloader\n","\n","        self.dl = get_dataloader(self.ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n","\n","        self.valid_dl = get_dataloader(self.valid_ds, batch_size = batch_size, shuffle = True, pad_to_longest = False, drop_last = True)\n","\n","        # prepare with accelerator\n","\n","        (\n","            self.mulan,\n","            self.optim,\n","            self.dl,\n","            self.valid_dl\n","        ) = self.accelerator.prepare(\n","            self.mulan,\n","            self.optim,\n","            self.dl,\n","            self.valid_dl\n","        )\n","\n","        # dataloader iterators\n","\n","        self.dl_iter = cycle(self.dl)\n","        self.valid_dl_iter = cycle(self.valid_dl)\n","\n","        self.save_model_every = save_model_every\n","\n","        hps = dict(\n","            num_train_steps = num_train_steps,\n","            data_max_length = data_max_length,\n","            learning_rate = lr\n","        )\n","\n","        self.accelerator.init_trackers(\"mulan\", config = hps)\n","\n","        # results folder\n","\n","        self.results_folder = Path(results_folder)\n","\n","        if force_clear_prev_results is True or (not exists(force_clear_prev_results) and len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?')):\n","            rmtree(str(self.results_folder))\n","\n","        self.results_folder.mkdir(parents = True, exist_ok = True)\n","\n","        # to device\n","\n","        self.mulan.to(self.device)\n","\n","    def save(self, path):\n","        pkg = dict(\n","            model = self.accelerator.get_state_dict(self.mulan),\n","            optim = self.optim.state_dict()\n","        )\n","        torch.save(pkg, path)\n","\n","    def load(self, path):\n","        path = Path(path)\n","        assert path.exists()\n","        pkg = torch.load(str(path), map_location = 'cpu')\n","\n","        mulan = self.accelerator.unwrap_model(self.mulan)\n","        mulan.load_state_dict(pkg['model'])\n","        self.optim.load_state_dict(pkg['optim'])\n","\n","    def print(self, msg):\n","        self.accelerator.print(msg)\n","\n","    @property\n","    def device(self):\n","        return self.accelerator.device\n","\n","    @property\n","    def is_distributed(self):\n","        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n","\n","    @property\n","    def is_main(self):\n","        return self.accelerator.is_main_process\n","\n","    @property\n","    def is_local_main(self):\n","        return self.accelerator.is_local_main_process\n","\n","    def data_tuple_to_kwargs(self, data):\n","        data_kwargs = {'wavs': data[0], 'texts': data[1]}\n","\n","        return data_kwargs\n","\n","    def train_step(self):\n","        device = self.device\n","\n","        steps = int(self.steps.item())\n","\n","        self.mulan.train()\n","\n","        # logs\n","\n","        logs = {}\n","\n","        # update vae (generator)\n","\n","        for _ in range(self.grad_accum_every):\n","            data_kwargs = self.data_tuple_to_kwargs(next(self.dl_iter))\n","\n","            loss = self.mulan(**data_kwargs)\n","\n","            self.accelerator.backward(loss / self.grad_accum_every)\n","\n","            accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n","\n","        if exists(self.max_grad_norm):\n","            self.accelerator.clip_grad_norm_(self.mulan.parameters(), self.max_grad_norm)\n","\n","        self.optim.step()\n","        self.optim.zero_grad()\n","\n","        # log\n","\n","        self.print(f\"{steps}: loss: {logs['loss']}\")\n","        self.accelerator.log({\"train_loss\": logs['loss']}, step = steps)\n","\n","        # save model every so often\n","\n","        if self.is_main and not (steps % self.save_model_every):\n","            model_path = str(self.results_folder / f'mulan.{steps}.pt')\n","            self.save(model_path)\n","\n","            self.print(f'{steps}: saving model to {str(self.results_folder)}')\n","\n","        self.steps += 1\n","        return logs\n","\n","    def train(self, log_fn: Callable = noop):\n","\n","        while self.steps < self.num_train_steps:\n","            logs = self.train_step()\n","            log_fn(logs)\n","\n","        self.print('training complete')"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:23:35.680677Z","iopub.status.busy":"2023-11-23T03:23:35.679262Z","iopub.status.idle":"2023-11-23T03:24:18.656348Z","shell.execute_reply":"2023-11-23T03:24:18.653201Z","shell.execute_reply.started":"2023-11-23T03:23:35.680638Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["training with dataset of 7214 samples and validating with randomly splitted 380 samples\n","0: loss: nan\n","0: saving model to results\n","1: loss: nan\n","2: loss: nan\n","3: loss: nan\n","4: loss: nan\n","5: loss: nan\n","5: saving model to results\n","6: loss: nan\n","7: loss: nan\n","8: loss: nan\n","9: loss: nan\n","training complete\n"]}],"source":["audio_transformer = AudioSpectrogramTransformer(**AUDIO_KWARGS)\n","\n","text_transformer = TextTransformer(**TEXT_KWARGS)\n","\n","mulan = MuLaN(audio_transformer, text_transformer)\n","\n","mulan_trainer = MuLaNTrainer(mulan, **MULAN_KWARGS)\n","\n","mulan_trainer.train()\n","\n","mulan_trainer.save(mulan_ckpt)\n","\n","del mulan_trainer"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Training SoundStream"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T03:24:18.663459Z","iopub.status.busy":"2023-11-23T03:24:18.662280Z","iopub.status.idle":"2023-11-23T03:24:19.831525Z","shell.execute_reply":"2023-11-23T03:24:19.828437Z","shell.execute_reply.started":"2023-11-23T03:24:18.663291Z"},"trusted":true},"outputs":[{"ename":"AssertionError","evalue":"only one Trainer can be instantiated at a time for training","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m soundstream \u001b[38;5;241m=\u001b[39m MusicLMSoundStream()\n\u001b[0;32m----> 2\u001b[0m soundstream_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSoundStreamTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoundstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mSOUNDSTREAM_TRAINER_KWARGS\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m soundstream_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m soundstream_trainer\u001b[38;5;241m.\u001b[39msave(soundstream_ckpt)\n","File \u001b[0;32m<@beartype(audiolm_pytorch.trainer.SoundStreamTrainer.__init__) at 0x79e74bdd1e10>:460\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_102373572347344, __beartype_object_134034319916096, __beartype_object_102373363221952, __beartype_object_102373453351520, __beartype_object_134034319915904, *args, **kwargs)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:200\u001b[0m, in \u001b[0;36mSoundStreamTrainer.__init__\u001b[0;34m(self, soundstream, num_train_steps, batch_size, data_max_length, data_max_length_seconds, folder, train_dataloader, val_dataloader, lr, grad_accum_every, wd, max_grad_norm, discr_max_grad_norm, save_results_every, save_model_every, log_losses_every, results_folder, valid_frac, random_split_seed, use_ema, ema_beta, ema_update_after_step, ema_update_every, apply_grad_penalty_every, dl_num_workers, accelerator, accelerate_kwargs, init_process_group_timeout_seconds, dataloader_drop_last, split_batches, use_wandb_tracking, force_clear_prev_results)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03mInitialize with a SoundStream instance and either a folder containing audio data or\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mtrain/val DataLoader instances.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 200\u001b[0m \u001b[43mcheck_one_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m accelerator\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (exists(accelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accelerate_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:64\u001b[0m, in \u001b[0;36mcheck_one_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_one_trainer\u001b[39m():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m ONE_TRAINER_INSTANTIATED\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ONE_TRAINER_INSTANTIATED, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly one Trainer can be instantiated at a time for training\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     65\u001b[0m     ONE_TRAINER_INSTANTIATED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: only one Trainer can be instantiated at a time for training"]}],"source":["soundstream = MusicLMSoundStream(target_sample_hz=48000)\n","soundstream_trainer = SoundStreamTrainer(\n","    soundstream,\n","    **SOUNDSTREAM_TRAINER_KWARGS\n",")\n","\n","soundstream_trainer.train()\n","\n","soundstream_trainer.save(soundstream_ckpt)\n","\n","del soundstream_trainer"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Training conditioning embeddings"]},{"cell_type":"markdown","metadata":{},"source":["Defining the MuLaN Embed Quantizer and Hubert K-means Embedder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.833724Z","iopub.status.idle":"2023-11-23T03:24:19.835065Z","shell.execute_reply":"2023-11-23T03:24:19.834506Z","shell.execute_reply.started":"2023-11-23T03:24:19.834421Z"},"trusted":true},"outputs":[],"source":["quantizer = MuLaNEmbedQuantizer(\n","    mulan=mulan,                         \n","    **MULAN_QUANTIZER_KWARGS\n",")\n","\n","wav2vec = HubertWithKmeans(\n","    **HUBERT_KWARGS\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Training Semantic Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.838800Z","iopub.status.idle":"2023-11-23T03:24:19.839468Z","shell.execute_reply":"2023-11-23T03:24:19.839159Z","shell.execute_reply.started":"2023-11-23T03:24:19.839129Z"},"trusted":true},"outputs":[],"source":["semantic_transformer = SemanticTransformer(\n","   num_semantic_tokens=wav2vec.codebook_size,\n","   **SEMANTIC_KWARGS \n",").to(DEVICE)\n","\n","semantic_trainer = SemanticTransformerTrainer(\n","    wav2vec,\n","    semantic_transformer,\n","    audio_conditioner=quantizer,\n","    **TRANSFORMER_TRAINER_KWARGS\n",")\n","\n","semantic_trainer.train()\n","\n","del semantic_trainer"]},{"cell_type":"markdown","metadata":{},"source":["Training Coarse Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.841735Z","iopub.status.idle":"2023-11-23T03:24:19.842565Z","shell.execute_reply":"2023-11-23T03:24:19.842054Z","shell.execute_reply.started":"2023-11-23T03:24:19.842025Z"},"trusted":true},"outputs":[],"source":["soundstream = MusicLMSoundStream()\n","\n","soundstream.load(soundstream_ckpt)\n","\n","coarse_transformer = CoarseTransformer(\n","    num_semantic_tokens=wav2vec.codebook_size,\n","    **COARSE_KWARGS\n",").to(DEVICE)\n","\n","coarse_trainer = CoarseTransformerTrainer(\n","    wav2vec,\n","    semantic_transformer,\n","    codec=soundstream,\n","    audio_conditioner=quantizer,\n","    **TRANSFORMER_TRAINER_KWARGS\n",")\n","\n","coarse_trainer.train()\n","\n","del coarse_trainer"]},{"cell_type":"markdown","metadata":{},"source":["Training Fine Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.845007Z","iopub.status.idle":"2023-11-23T03:24:19.845609Z","shell.execute_reply":"2023-11-23T03:24:19.845419Z","shell.execute_reply.started":"2023-11-23T03:24:19.845397Z"},"trusted":true},"outputs":[],"source":["soundstream = MusicLMSoundStream()\n","\n","soundstream.load(soundstream_ckpt)\n","\n","fine_transformer = FineTransformer(\n","    codebook_size=wav2vec.codebook_size,\n","    **FINE_KWARGS\n",").to(DEVICE)\n","\n","fine_trainer = FineTransformerTrainer(\n","    wav2vec,\n","    semantic_transformer,\n","    codec=soundstream\n","    audio_conditioner=quantizer,\n","    **TRANSFORMER_TRAINER_KWARGS\n",")\n","\n","fine_trainer.train()\n","\n","del fine_trainer"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Combining AudioLM and MusicLM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.848530Z","iopub.status.idle":"2023-11-23T03:24:19.848972Z","shell.execute_reply":"2023-11-23T03:24:19.848783Z","shell.execute_reply.started":"2023-11-23T03:24:19.848762Z"},"trusted":true},"outputs":[],"source":["audio_lm = AudioLM(\n","    wav2vec=wav2vec,\n","    codec=soundstream,\n","    semantic_transformer=semantic_transformer,\n","    coarse_transformer=coarse_transformer,\n","    fine_transformer=fine_transformer   \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.851148Z","iopub.status.idle":"2023-11-23T03:24:19.852276Z","shell.execute_reply":"2023-11-23T03:24:19.852047Z","shell.execute_reply.started":"2023-11-23T03:24:19.852024Z"},"trusted":true},"outputs":[],"source":["music_lm = MusicLM(\n","    audio_lm=audio_lm,\n","    mulan_embed_quantizer=quantizer\n",")\n","\n","music = music_lm('Café Au Lait 1970s', num_samples=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.854502Z","iopub.status.idle":"2023-11-23T03:24:19.855034Z","shell.execute_reply":"2023-11-23T03:24:19.854842Z","shell.execute_reply.started":"2023-11-23T03:24:19.854821Z"},"trusted":true},"outputs":[],"source":["torch.save(music, 'generated_music.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-23T03:24:19.858070Z","iopub.status.idle":"2023-11-23T03:24:19.858682Z","shell.execute_reply":"2023-11-23T03:24:19.858338Z","shell.execute_reply.started":"2023-11-23T03:24:19.858317Z"},"trusted":true},"outputs":[],"source":["output_path = \"out.wav\"\n","sample_rate = 44100\n","torchaudio.save(output_path, music.cpu(), sample_rate)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4043676,"sourceId":7030475,"sourceType":"datasetVersion"},{"datasetId":3918425,"sourceId":7031158,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
